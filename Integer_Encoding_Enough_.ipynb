{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Is One-Hot Encoding Really Necessary?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](http://oi65.tinypic.com/2nvf248.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a specific project, I can only Integer Encode, but not One-Hot Encode.\n",
    "\n",
    "Many Machine Learning experts such as Sebastian Raschka (\"Python Machine Learning\") urge coders to do one-hot encoding. The main argument is that for categorical labels without an order, Integer Encoding can screw up your model. But there are also some disadvantages (introduction of correlation, overfitting...).\n",
    "\n",
    "I did not want to look stupid in front of my customer, so I did what any reasonable ML coder does: I created my own dataset to test it.\n",
    "\n",
    "SYNTHETIC DATASET:\n",
    "- 50 sales people\n",
    "- A - H features (random variables ranging from 1-100)\n",
    "- 20 examples (i.e each sales is represented 20 times).\n",
    "- 1 sales person always with label 1.\n",
    "\n",
    "The hypothesis is, that if I only Integer Encode each sales (sales 1 = 1, sales 2 = 2...), the model should struggle because sales people are not ordinal (i.e. sales 40 is not better than sales 4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Difference between Integer Encoding vs. One-Hot Encoding\n",
    "\n",
    "Logistic Regression: \n",
    "Integer Encoding: \n",
    "- Precision 1:  67%\n",
    "- Recall 1: 100%\n",
    "\n",
    "One-Hot Encoding:\n",
    "- Precision 1: 67%\n",
    "- Recall 1: 100%\n",
    "\n",
    "Decision Tree Classifier: \n",
    "Integer Encoding: \n",
    "- Precision 1:  80%\n",
    "- Recall 1: 100%\n",
    "\n",
    "One-Hot Encoding:\n",
    "- Precision 1: 100% !!!\n",
    "- Recall 1: 100%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion:\n",
    "Logistic Regression seems not to care about One-Hot Encoding, but the Decision Tree clearly does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary libraries from Sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithms\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"One_Hot_Test.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>E</th>\n",
       "      <th>F</th>\n",
       "      <th>G</th>\n",
       "      <th>H</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Deadra</td>\n",
       "      <td>90</td>\n",
       "      <td>56</td>\n",
       "      <td>47</td>\n",
       "      <td>72</td>\n",
       "      <td>80</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Frances</td>\n",
       "      <td>37</td>\n",
       "      <td>66</td>\n",
       "      <td>66</td>\n",
       "      <td>47</td>\n",
       "      <td>84</td>\n",
       "      <td>70</td>\n",
       "      <td>22</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Perry</td>\n",
       "      <td>45</td>\n",
       "      <td>64</td>\n",
       "      <td>70</td>\n",
       "      <td>38</td>\n",
       "      <td>15</td>\n",
       "      <td>91</td>\n",
       "      <td>21</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ronny</td>\n",
       "      <td>100</td>\n",
       "      <td>50</td>\n",
       "      <td>48</td>\n",
       "      <td>76</td>\n",
       "      <td>22</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Terry</td>\n",
       "      <td>37</td>\n",
       "      <td>59</td>\n",
       "      <td>79</td>\n",
       "      <td>25</td>\n",
       "      <td>38</td>\n",
       "      <td>30</td>\n",
       "      <td>54</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Name    A   B   C   D   E   F   G   H  Label\n",
       "0   Deadra     90  56  47  72  80  64  10  24      0\n",
       "1  Frances     37  66  66  47  84  70  22  33      0\n",
       "2    Perry     45  64  70  38  15  91  21  35      0\n",
       "3    Ronny    100  50  48  76  22   8  11  69      0\n",
       "4    Terry     37  59  79  25  38  30  54  27      0"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 10)"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label Encoder (not one-hot encoding!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.fit(data['Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "Name_ = le.transform(data['Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(Name_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns.names = ['Name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat([data, df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.drop(['Name', 'Label'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_size = 0.2\n",
    "seed = 12\n",
    "X_train, X_test, y_train, y_test, = train_test_split(X, y, test_size=validation_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_fold = 10\n",
    "kfold = KFold(n_splits=10, random_state=12)\n",
    "seed = 12 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOGISTIC REGRESSION\n",
    "model = LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True, \n",
    "                           intercept_scaling=1, class_weight=\"balanced\", random_state=12, \n",
    "                           solver='warn', max_iter=100, multi_class='warn', verbose=0, \n",
    "                           warm_start=False, n_jobs=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DECISION TREE\n",
    "model = DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=3, min_samples_split=2,\n",
    "                               min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None,\n",
    "                               random_state=12, max_leaf_nodes=None, min_impurity_decrease=0.0, \n",
    "                               min_impurity_split=None, class_weight=\"balanced\", presort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='warn', n_jobs=None, penalty='l2', random_state=12,\n",
       "          solver='warn', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions for test data\n",
    "y_pred = model.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99       196\n",
      "           1       0.67      1.00      0.80         4\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       200\n",
      "   macro avg       0.83      0.99      0.90       200\n",
      "weighted avg       0.99      0.99      0.99       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "predicted = model.predict(X_test)\n",
    "report = classification_report(y_test, predicted)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = pd.concat([data.drop('Name', axis=1), pd.get_dummies(data['Name'])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2.drop(['Label'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2 = data['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_size = 0.2\n",
    "seed = 12\n",
    "X2_train, X2_test, y2_train, y2_test, = train_test_split(X2, y2, test_size=validation_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_fold = 10\n",
    "kfold = KFold(n_splits=10, random_state=12)\n",
    "seed = 12 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOGISTIC REGRESSION\n",
    "model_2 = LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True, \n",
    "                           intercept_scaling=1, class_weight=\"balanced\", random_state=12, \n",
    "                           solver='warn', max_iter=100, multi_class='warn', verbose=0, \n",
    "                           warm_start=False, n_jobs=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DECISION TREE\n",
    "model_2 = DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=3, min_samples_split=2,\n",
    "                               min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None,\n",
    "                               random_state=12, max_leaf_nodes=None, min_impurity_decrease=0.0, \n",
    "                               min_impurity_split=None, class_weight=\"balanced\", presort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='warn', n_jobs=None, penalty='l2', random_state=12,\n",
       "          solver='warn', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2.fit(X2_train, y2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions for test data\n",
    "y2_pred = model_2.predict(X2_test)\n",
    "predictions = [round(value) for value in y2_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99       196\n",
      "           1       0.67      1.00      0.80         4\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       200\n",
      "   macro avg       0.83      0.99      0.90       200\n",
      "weighted avg       0.99      0.99      0.99       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "predicted = model_2.predict(X2_test)\n",
    "report = classification_report(y2_test, predicted)\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py36)",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
